Use GPU: 0 for training
==> loading teacher model
==> done
==> training...
Epoch: [1][0/378]	GPU 0	Time: 3.212	Loss 0.0000	Acc@1 60.938	Acc@5 100.000	
Epoch: [1][200/378]	GPU 0	Time: 51.481	Loss 0.0000	Acc@1 68.113	Acc@5 100.000	
 * Epoch 1, GPU 0, Acc@1 68.306, Acc@5 100.000
GPU 0 validating
Test: [0/95]	GPU: 0	Time: 0.915	Loss 0.6469	Acc@1 67.188	Acc@5 100.000	
 ** Acc@1 68.359, Acc@5 100.000
saving the best model!
==> training...
Epoch: [2][0/378]	GPU 0	Time: 1.151	Loss 0.0000	Acc@1 75.000	Acc@5 100.000	
Epoch: [2][200/378]	GPU 0	Time: 48.394	Loss 0.0000	Acc@1 68.649	Acc@5 100.000	
Traceback (most recent call last):
  File "train_student.py", line 467, in <module>
    main()
  File "train_student.py", line 180, in main
    main_worker(None if ngpus_per_node > 1 else opt.gpu_id, ngpus_per_node, opt)
  File "train_student.py", line 385, in main_worker
    train_acc, train_acc_top5, train_loss = train(epoch, train_loader, module_list, criterion_list, optimizer, opt)
  File "/content/drive/.shortcut-targets-by-id/1aAWk40Gy3z7CoH8LjUDuJOtYT0RfUzYr/CSE499/SimKD/helper/loops.py", line 113, in train_distill
    images = images.cuda(opt.gpu if opt.multiprocessing_distributed else 0, non_blocking=True)
KeyboardInterrupt